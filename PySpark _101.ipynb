{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d0e2cd",
   "metadata": {
    "papermill": {
     "duration": 0.00253,
     "end_time": "2024-08-10T16:27:44.118788",
     "exception": false,
     "start_time": "2024-08-10T16:27:44.116258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Apache Spark\n",
    "\n",
    "Apache Spark is an open-source, unified analytics engine for large-scale data processing and machine learning. It provides high-level APIs in Java, Python, Scala, and R, and an optimized engine that supports general computation graphs for data analysis.\n",
    "\n",
    "### Key Features:\n",
    "- Speed: Spark is designed for speed, with the ability to process data up to 100 times faster than traditional big data technologies.\n",
    "- Ease of Use: Spark provides high-level APIs and a simple programming model, making it easy to write applications.\n",
    "- Flexibility: Spark can handle batch processing, stream processing, machine learning, and graph processing, all in a single platform.\n",
    "- Unified Engine: Spark's engine is designed to handle multiple workloads, making it a versatile tool for data analysis.\n",
    "\n",
    "### Components:\n",
    "- Spark Core: The foundation of Spark, providing basic data structures and APIs.\n",
    "- Spark SQL: A module for structured data processing, with support for SQL queries.\n",
    "- Spark Streaming: A module for real-time data processing.\n",
    "- MLlib: A machine learning library, providing algorithms for classification, regression, clustering, and more.\n",
    "- GraphX: A module for graph processing and analytics.\n",
    "\n",
    "# PySpark\n",
    "\n",
    "PySpark is the Python API for Apache Spark, allowing Python developers to write Spark applications using Python. It provides a seamless integration with Spark's engine, enabling data processing, machine learning, and data analytics.\n",
    "Key Features:\n",
    "- Pythonic API: PySpark provides a Python-friendly API, making it easy to write Spark applications.\n",
    "- Dynamic Typing: PySpark supports dynamic typing, allowing for flexible data processing.\n",
    "- Integration with Spark: PySpark is built on top of Spark's engine, providing access to Spark's features and performance.\n",
    "- DataFrames and Datasets: PySpark supports DataFrames and Datasets, providing a structured data processing API.\n",
    "- Machine Learning: PySpark provides access to Spark's MLlib, enabling machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b29df",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001553,
     "end_time": "2024-08-10T16:27:44.122452",
     "exception": false,
     "start_time": "2024-08-10T16:27:44.120899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.609869,
   "end_time": "2024-08-10T16:27:44.545274",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-10T16:27:40.935405",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
